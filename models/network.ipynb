{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 12:47:04.352110: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/lib/native\n",
      "2024-03-20 12:47:04.971686: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/lib/native\n",
      "2024-03-20 12:47:04.971744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/lib/native\n",
      "2024-03-20 12:47:04.971749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from generator import RandomGraphDataset\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, validation_dataset=None, optimizer=None, epochs=10, batch_size=5):\n",
    "    x_loss_weight = 0.5\n",
    "    h_loss_weight = 1 - x_loss_weight\n",
    "\n",
    "    loss_edges_train, loss_reach_train, loss_parents_train = [], [], []\n",
    "    loss_edges_val, loss_reach_val, loss_parents_val = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_count = len(train_dataset) // batch_size\n",
    "        \n",
    "        cumulated_loss_edges_epoch, cumulated_loss_reach_epoch, cumulated_loss_parents_epoch = 0, 0, 0\n",
    "\n",
    "        for i in range(batch_count):\n",
    "            model.train()\n",
    "            cumulated_loss_edges, cumulated_loss_reach, cumulated_loss_parents = 0, 0, 0\n",
    "            for j in range(i*batch_size, (i+1)*batch_size):\n",
    "                graph = train_dataset[j] \n",
    "                loss_edges, loss_reach, loss_parents = model(graph)\n",
    "                loss_edges_output, loss_edges_hints = loss_edges[0], loss_edges[1] # loss for the edges\n",
    "\n",
    "                cumulated_loss_edges += x_loss_weight * loss_edges_output + h_loss_weight * loss_edges_hints\n",
    "                cumulated_loss_reach += loss_reach\n",
    "                cumulated_loss_parents += loss_parents\n",
    "\n",
    "            cumulated_loss_edges /= batch_size\n",
    "            cumulated_loss_reach /= batch_size\n",
    "            cumulated_loss_parents /= batch_size\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            cumulated_loss_edges.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cumulated_loss_edges_epoch += cumulated_loss_edges\n",
    "            cumulated_loss_reach_epoch += cumulated_loss_reach\n",
    "            cumulated_loss_parents_epoch += cumulated_loss_parents\n",
    "\n",
    "        # Convert tensors to lists and append to the respective lists\n",
    "        loss_edges_train.append(cumulated_loss_edges_epoch.item() / batch_count)\n",
    "        loss_reach_train.append(cumulated_loss_reach_epoch.item() / batch_count)\n",
    "        loss_parents_train.append(cumulated_loss_parents_epoch.item() / batch_count)\n",
    "\n",
    "        if validation_dataset:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumulated_loss_edges_val, cumulated_loss_reach_val, cumulated_loss_parents_val = 0, 0, 0\n",
    "                for k in range(len(validation_dataset)):\n",
    "                    graph = validation_dataset[k]\n",
    "                    loss_edges, loss_reach, loss_parents = model(graph)\n",
    "                    loss_edges_output, loss_edges_hints = loss_edges[0], loss_edges[1] # loss for the edges\n",
    "\n",
    "                    cumulated_loss_edges_val += x_loss_weight * loss_edges_output + h_loss_weight * loss_edges_hints\n",
    "                    cumulated_loss_reach_val += loss_reach\n",
    "                    cumulated_loss_parents_val += loss_parents \n",
    "\n",
    "                cumulated_loss_edges_val /= len(validation_dataset)\n",
    "                cumulated_loss_reach_val /= len(validation_dataset)\n",
    "                cumulated_loss_parents_val /= len(validation_dataset)\n",
    "\n",
    "                loss_edges_val.append(cumulated_loss_edges_val.item())\n",
    "                loss_reach_val.append(cumulated_loss_reach_val.item())\n",
    "                loss_parents_val.append(cumulated_loss_parents_val.item())\n",
    "\n",
    "                print(f'Epoch {epoch}, loss_edges {cumulated_loss_edges_epoch.item() / batch_count}, loss_reach {cumulated_loss_reach_epoch.item() / batch_count}, loss_parents {cumulated_loss_parents_epoch.item() / batch_count}, loss_edges_val {cumulated_loss_edges_val.item()}, loss_reach_val {cumulated_loss_reach_val.item()}, loss_parents_val {cumulated_loss_parents_val.item()}')\n",
    "        \n",
    "        else:\n",
    "            print(f'Epoch {epoch}, loss_edges {cumulated_loss_edges_epoch.item() / batch_count}, loss_reach {cumulated_loss_reach_epoch.item() / batch_count}, loss_parents {cumulated_loss_parents_epoch.item() / batch_count}')\n",
    "\n",
    "    if validation_dataset:\n",
    "        return loss_edges_train, loss_reach_train, loss_parents_train, loss_edges_val, loss_reach_val, loss_parents_val\n",
    "    return loss_edges_train, loss_reach_train, loss_parents_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "n=[20, 100]\n",
    "p=0.3\n",
    "dataset = RandomGraphDataset(root='./data/medium', gen_num_graph=400, n=n, p=p)\n",
    "train_dataset, test_dataset = random_split(dataset, [350, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "model = Network()\n",
    "lr = 0.0002\n",
    "\n",
    "loss_edges_train, loss_reach_train, loss_parents_train = train(model=model, train_dataset=train_dataset,\n",
    "      optimizer=optimizer(model.parameters(), lr=lr), epochs=80, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "model = Network()\n",
    "lr = 0.0002\n",
    "\n",
    "loss_edges_train, loss_reach_train, loss_parents_train, loss_edges_val, loss_reach_val, loss_parents_val = train(model=model, train_dataset=train_dataset, validation_dataset=test_dataset,\n",
    "      optimizer=optimizer(model.parameters(), lr=lr), epochs=120, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_0002_350_120.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RandomGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing using the train_test_split function\n",
    "from torch.utils.data import random_split\n",
    "n=[200, 500]\n",
    "p=0.3\n",
    "dataset_test = RandomGraphDataset(root='./data/OOD/200', gen_num_graph=34, n=n, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loss_weight = h_loss_weight = 0.5\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    cumulated_loss_edges_test, cumulated_loss_reach_test, cumulated_loss_parents_test = 0, 0, 0\n",
    "    for graph in dataset_test:\n",
    "        loss_edges, loss_reach, loss_parents = model(graph)\n",
    "        loss_edges_output, loss_edges_hints = loss_edges[0], loss_edges[1]  # loss for the edges\n",
    "\n",
    "        cumulated_loss_edges_test += x_loss_weight * loss_edges_output + h_loss_weight * loss_edges_hints\n",
    "        cumulated_loss_reach_test += loss_reach\n",
    "        cumulated_loss_parents_test += loss_parents\n",
    "\n",
    "    cumulated_loss_edges_test /= len(dataset_test)\n",
    "    cumulated_loss_reach_test /= len(dataset_test)\n",
    "    cumulated_loss_parents_test /= len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulated_loss_edges_test, cumulated_loss_reach_test, cumulated_loss_parents_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
