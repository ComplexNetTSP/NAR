{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import generator from '../data/generator.py'\n",
    "import sys\n",
    "import os\n",
    "from generator import RandomGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from mpnn import MPNN\n",
    "from torch.functional import F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "        self.encoder = Encoder(2, latent_dim)\n",
    "        self.processor = MPNN(latent_dim*2, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, 1)\n",
    "\n",
    "    def forward(self, batch, max_iter=10):\n",
    "        input = torch.stack((batch.pos, batch.s), dim=1).float()\n",
    "        h = torch.zeros(input.size(0), 128) # hidden state from the processor\n",
    "        hints_edges = batch.edges_h[1:] # hints if an edge was passed or not\n",
    "        true_output = batch.edges # true_output for all the edges if they were passed or not at the end.\n",
    "        max_iter = hints_edges.size(0)\n",
    "\n",
    "        hints_reach = batch.reach_h[1:] # hints from the reachability\n",
    "        \n",
    "        predictions_edges = torch.zeros(max_iter, batch.edges.size(0))\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            threshold = 0.5\n",
    "\n",
    "            # NETWORK START\n",
    "            z = self.encoder(input) # the encoded input\n",
    "            processor_input = torch.cat([z, h], dim=1) # the input to the processor\n",
    "            h = self.processor(processor_input, batch.edge_index.long()) # the output of the processor\n",
    "            decoder_input = torch.cat((h[batch.edge_index[0]], h[batch.edge_index[1]]), dim=1)\n",
    "            alpha = self.decoder(decoder_input).view(batch.edges.size(0))\n",
    "            # NETWORK END\n",
    "\n",
    "            # predictions for the edges\n",
    "            predictions_edges[i] = alpha.view(batch.edges.size(0))\n",
    "            # update the input with the new state\n",
    "            predictions_reach = self.calculate_reach(batch, alpha, threshold=0.4)\n",
    "            \n",
    "            input = torch.stack((batch.pos, predictions_reach), dim=1).float()\n",
    "\n",
    "        # predictions for reach \n",
    "\n",
    "        # predictions for the parents\n",
    "        predictions_parents = self.get_parent_nodes(batch.edge_index, alpha, batch.s, threshold=0.)\n",
    "        \n",
    "        loss_edges = self.calculate_loss(hints_edges, predictions_edges, true_output)\n",
    "        loss_reach = self.calculate_reach_loss(predictions_reach, batch.reach_h[-1])\n",
    "        loss_parents = self.calculate_parents_loss(predictions_parents, batch.pi)\n",
    "        \n",
    "        return loss_edges, loss_reach, loss_parents\n",
    "        \n",
    "    def calculate_loss(self, hints, predictions, true_output):\n",
    "        if len(predictions) == 0:\n",
    "            return torch.tensor(0.0), torch.tensor(0.0)  # Return zero loss if predictions is empty\n",
    "        loss_x = F.binary_cross_entropy(predictions[-1], true_output.type(torch.float))\n",
    "        loss_h = 0\n",
    "        for i in range(hints.size(0)):\n",
    "            loss_h += F.binary_cross_entropy(predictions[i], hints[i].type(torch.float))\n",
    "        return loss_x, loss_h\n",
    "    \n",
    "    def calculate_reach_loss(self, predictions, true_output):\n",
    "        return F.binary_cross_entropy(predictions, true_output.type(torch.float))\n",
    "    \n",
    "\n",
    "    def calculate_reach(self, graph, alpha, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Calculate reachability values for each node from the alpha values\n",
    "\n",
    "        Args:\n",
    "        - graph: Graph object containing graph data\n",
    "        - alpha: PyTorch tensor containing alpha values\n",
    "        - threshold: Threshold value for reachability\n",
    "\n",
    "        Returns:\n",
    "        - y: PyTorch tensor containing reachability values for each node\n",
    "        \"\"\"\n",
    "        y = torch.zeros((len(graph.s)))\n",
    "        for node_index in range(len(graph.s)):\n",
    "            # Check if there are any edges connected to the current node\n",
    "            connected_edges = torch.logical_or(graph.edge_index[0] == node_index, graph.edge_index[1] == node_index)\n",
    "            if torch.any(connected_edges):\n",
    "                alpha_max_proba = alpha[connected_edges].max()\n",
    "                if alpha_max_proba.item() >= threshold:\n",
    "                    y[node_index] = 1\n",
    "        return y\n",
    "\n",
    "\n",
    "    def get_parent_nodes(self, edge_index, alpha, s, threshold=0.8):\n",
    "        num_nodes = len(s)\n",
    "        parent_nodes = torch.arange(num_nodes)  # Initialize parent nodes with their own index\n",
    "\n",
    "        for node in range(num_nodes):\n",
    "            # Get all edges that point to the current node\n",
    "            incoming_edges = (edge_index[1] == node).nonzero(as_tuple=False).squeeze()\n",
    "            \n",
    "            # Check if there are any edges pointing to the current node\n",
    "            if incoming_edges.numel() != 0:\n",
    "                # Filter alpha based on threshold\n",
    "                filtered_edges = incoming_edges[alpha[incoming_edges] >= threshold]\n",
    "                \n",
    "                # Check if there are any edges left after filtering\n",
    "                if filtered_edges.numel() != 0:\n",
    "                    # Get the index of the edge with the highest alpha after filtering\n",
    "                    max_alpha_index = torch.argmax(alpha[filtered_edges])\n",
    "                    # Get the parent node\n",
    "                    parent_nodes[node] = edge_index[0, filtered_edges[max_alpha_index]].item()\n",
    "\n",
    "        return parent_nodes\n",
    "\n",
    "    def calculate_parents_loss(self, predictions_parents, final_parents):\n",
    "        if len(predictions_parents) == 0 or len(final_parents) == 0:\n",
    "            return torch.tensor(1.0) \n",
    "        return 1 - torch.mean(torch.eq(predictions_parents, final_parents).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, validation_dataset=None, optimizer=None, epochs=10, batch_size=5):\n",
    "    x_loss_weight = 0.5\n",
    "    h_loss_weight = 1 - x_loss_weight\n",
    "\n",
    "    loss_edges_train, loss_reach_train, loss_parents_train = [], [], []\n",
    "    loss_edges_val, loss_reach_val, loss_parents_val = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_count = len(train_dataset) // batch_size\n",
    "        \n",
    "        cumulated_loss_edges_epoch, cumulated_loss_reach_epoch, cumulated_loss_parents_epoch = 0, 0, 0\n",
    "\n",
    "        for i in range(batch_count):\n",
    "            model.train()\n",
    "            cumulated_loss_edges, cumulated_loss_reach, cumulated_loss_parents = 0, 0, 0\n",
    "            for j in range(i*batch_size, (i+1)*batch_size):\n",
    "                graph = train_dataset[j] \n",
    "                loss_edges, loss_reach, loss_parents = model(graph)\n",
    "                loss_edges_output, loss_edges_hints = loss_edges[0], loss_edges[1] # loss for the edges\n",
    "\n",
    "                cumulated_loss_edges += x_loss_weight * loss_edges_output + h_loss_weight * loss_edges_hints\n",
    "                cumulated_loss_reach += loss_reach\n",
    "                cumulated_loss_parents += loss_parents\n",
    "\n",
    "            cumulated_loss_edges /= batch_size\n",
    "            cumulated_loss_reach /= batch_size\n",
    "            cumulated_loss_parents /= batch_size\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            cumulated_loss_edges.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            cumulated_loss_edges_epoch += cumulated_loss_edges\n",
    "            cumulated_loss_reach_epoch += cumulated_loss_reach\n",
    "            cumulated_loss_parents_epoch += cumulated_loss_parents\n",
    "\n",
    "        # Convert tensors to lists and append to the respective lists\n",
    "        loss_edges_train.append(cumulated_loss_edges_epoch.item() / batch_count)\n",
    "        loss_reach_train.append(cumulated_loss_reach_epoch.item() / batch_count)\n",
    "        loss_parents_train.append(cumulated_loss_parents_epoch.item() / batch_count)\n",
    "\n",
    "        if validation_dataset:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumulated_loss_edges_val, cumulated_loss_reach_val, cumulated_loss_parents_val = 0, 0, 0\n",
    "                for k in range(len(validation_dataset)):\n",
    "                    graph = validation_dataset[k]\n",
    "                    loss_edges, loss_reach, loss_parents = model(graph)\n",
    "                    loss_edges_output, loss_edges_hints = loss_edges[0], loss_edges[1] # loss for the edges\n",
    "\n",
    "                    cumulated_loss_edges_val += x_loss_weight * loss_edges_output + h_loss_weight * loss_edges_hints\n",
    "                    cumulated_loss_reach_val += loss_reach\n",
    "                    cumulated_loss_parents_val += loss_parents \n",
    "\n",
    "                cumulated_loss_edges_val /= len(validation_dataset)\n",
    "                cumulated_loss_reach_val /= len(validation_dataset)\n",
    "                cumulated_loss_parents_val /= len(validation_dataset)\n",
    "\n",
    "                loss_edges_val.append(cumulated_loss_edges_val.item())\n",
    "                loss_reach_val.append(cumulated_loss_reach_val.item())\n",
    "                loss_parents_val.append(cumulated_loss_parents_val.item())\n",
    "\n",
    "                print(f'Epoch {epoch}, loss_edges {cumulated_loss_edges_epoch.item() / batch_count}, loss_reach {cumulated_loss_reach_epoch.item() / batch_count}, loss_parents {cumulated_loss_parents_epoch.item() / batch_count}, loss_edges_val {cumulated_loss_edges_val.item()}, loss_reach_val {cumulated_loss_reach_val.item()}, loss_parents_val {cumulated_loss_parents_val.item()}')\n",
    "        \n",
    "        else:\n",
    "            print(f'Epoch {epoch}, loss_edges {cumulated_loss_edges_epoch.item() / batch_count}, loss_reach {cumulated_loss_reach_epoch.item() / batch_count}, loss_parents {cumulated_loss_parents_epoch.item() / batch_count}')\n",
    "\n",
    "    if validation_dataset:\n",
    "        return loss_edges_train, loss_reach_train, loss_parents_train, loss_edges_val, loss_reach_val, loss_parents_val\n",
    "    return loss_edges_train, loss_reach_train, loss_parents_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing using the train_test_split function\n",
    "from torch.utils.data import random_split\n",
    "n=[20, 100]\n",
    "p=0.3\n",
    "dataset = RandomGraphDataset(root='./data/medium', gen_num_graph=400, n=n, p=p)\n",
    "train_dataset, test_dataset = random_split(dataset, [350, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "model = Network()\n",
    "lr = 0.0002\n",
    "\n",
    "loss_edges_train, loss_reach_train, loss_parents_train = train(model=model, train_dataset=train_dataset,\n",
    "      optimizer=optimizer(model.parameters(), lr=lr), epochs=80, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "model = Network()\n",
    "lr = 0.0002\n",
    "\n",
    "loss_edges_train, loss_reach_train, loss_parents_train, loss_edges_val, loss_reach_val, loss_parents_val = train(model=model, train_dataset=train_dataset, validation_dataset=test_dataset,\n",
    "      optimizer=optimizer(model.parameters(), lr=lr), epochs=80, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network().forward(input, dataset[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_parents_val, label='val')\n",
    "plt.plot(loss_edges_train, label='train')\n",
    "\n",
    "plt.legend()\n",
    "# use log scale\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_edges_val, label='val')\n",
    "plt.plot(loss_edges_train, label='train')\n",
    "\n",
    "plt.legend()\n",
    "# use log scale\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_reach_val, label='val')\n",
    "plt.plot(loss_reach_train, label='train')\n",
    "\n",
    "plt.legend()\n",
    "# use log scale\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_parents_val, label='train')\n",
    "plt.plot(loss_reach_val, label='train')\n",
    "plt.plot(loss_edges_val, label='train')\n",
    "plt.legend()\n",
    "# use log scale\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_edges_train, label='train')\n",
    "plt.legend()\n",
    "# use log scale\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import RandomGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing using the train_test_split function\n",
    "from torch.utils.data import random_split\n",
    "n=[200, 500]\n",
    "p=0.3\n",
    "dataset = RandomGraphDataset(root='./data/OOD/200', gen_num_graph=34, n=n, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
