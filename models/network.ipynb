{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 11:04:40.777734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import generator from '../data/generator.py'\n",
    "import sys\n",
    "import os\n",
    "from generator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 200 graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 567.93it/s]\n",
      "Processing...\n",
      "100%|██████████| 200/200 [00:00<00:00, 332.04it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "p = 0.3\n",
    "\n",
    "dataset = RandomGraphDataset(root='./data', gen_num_graph=200, n=n, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from mpnn import MPNN\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "        self.encoder = Encoder(2, latent_dim)\n",
    "        self.processor = MPNN(latent_dim*2, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encoder(x)\n",
    "        h = torch.zeros(x.size(0), 128)\n",
    "        processor_input = torch.cat([z, h], dim=1)\n",
    "        x = self.processor(processor_input, edge_index)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "    def forward(self, batch:torch.tensor, batch_pred: torch.Tensor):\n",
    "\n",
    "        # for every batch find the predicted and true values and send them to the calculate_loss function\n",
    "        loss_x = 0\n",
    "        loss_h = 0\n",
    "        for i in range(batch.size(0)):\n",
    "            data = batch[i]\n",
    "            x = data.reach_h[-1] # true output value\n",
    "            x_pred = batch_pred[i][-1] # predicted output value\n",
    "            h_pred = batch_pred[i][:-2] # predicted hint values\n",
    "            h = data.reach_h[:len(h_pred)] # true hint values\n",
    "            loss_x += F.binary_cross_entropy(x, x_pred)\n",
    "            print(loss_x)\n",
    "            for i in range(h.size(1)):\n",
    "                loss_h += F.binary_cross_entropy(h[:, i], h_pred[:, i])\n",
    "\n",
    "        return loss_x, loss_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from mpnn import MPNN\n",
    "from loss import Loss\n",
    "from torch.functional import F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Network, self).__init__()\n",
    "        self.encoder = Encoder(2, latent_dim)\n",
    "        self.processor = MPNN(latent_dim*2, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, 1)\n",
    "\n",
    "    def forward(self, batch, max_iter=10):\n",
    "        input = torch.stack((batch.pos, batch.s), dim=1).float()\n",
    "        h = torch.zeros(input.size(0), 128) # hidden state from the processor\n",
    "        hints = batch.reach_h[1:] # hints from the reachability\n",
    "        true_output = batch.reach_h[-1] # true_output expected from the reachability\n",
    "        max_iter = hints.size(0)+1\n",
    "        predictions = torch.zeros(max_iter, input.size(0))\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            z = self.encoder(input) # the encoded input\n",
    "            processor_input = torch.cat([z, h], dim=1) # the input to the processor\n",
    "            h = self.processor(processor_input, batch.edge_index.long()) # the output of the processor\n",
    "            y = self.decoder(h).view(batch.s.size())   # decoded state from the processor\n",
    "            predictions[i] = y\n",
    "            input = torch.stack((batch.pos, y), dim=1).float() # we update the input with the new state\n",
    "        \n",
    "        loss = self.calculate_loss(hints, predictions, true_output)\n",
    "        return y, loss\n",
    "    \n",
    "    def calculate_loss(self, hints, predictions, true_output):\n",
    "        loss_x = F.binary_cross_entropy(torch.sigmoid(predictions[-1]), true_output.type(torch.float))\n",
    "        loss_h = 0\n",
    "        for i in range(hints.size(0)):\n",
    "            loss_h += F.binary_cross_entropy(torch.sigmoid(predictions[i]), hints[i].type(torch.float))\n",
    "        return loss_x, loss_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, validation_dataset=None, optimizer=None, epochs=10, batch_size=5):\n",
    "    x_loss_weight = 0.5\n",
    "    h_loss_weight = 1 - x_loss_weight\n",
    "    for epoch in range(epochs):\n",
    "        batch_count = len(train_dataset) // batch_size\n",
    "        for i in range(batch_count):\n",
    "            model.train()\n",
    "            cumulated_loss = 0\n",
    "            for j in range(i*batch_size, (i+1)*batch_size):\n",
    "                graph = train_dataset[j] \n",
    "                y, loss = model(graph)\n",
    "                loss_x = loss[0] # loss for the output\n",
    "                loss_hints = loss[1] # loss for the hints\n",
    "                cumulated_loss += x_loss_weight * loss_x + h_loss_weight * loss_hints # we combine the two losses\n",
    "            \n",
    "            cumulated_loss /= batch_size\n",
    "            optimizer.zero_grad()\n",
    "            cumulated_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if validation_dataset:\n",
    "            model.eval()\n",
    "            cumulated_loss_val = 0\n",
    "            with torch.no_grad():\n",
    "                for k in range(len(validation_dataset)):\n",
    "                    graph = validation_dataset[k]\n",
    "                    y, loss = model(graph)\n",
    "                    loss_x, loss_hints = loss[0], loss[1]\n",
    "                    cumulated_loss_val += x_loss_weight * loss_x + h_loss_weight * loss_hints\n",
    "\n",
    "            cumulated_loss_val /= len(validation_dataset)\n",
    "\n",
    "            print(f'Epoch {epoch}, loss {cumulated_loss.item()}, validation loss {cumulated_loss_val}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch}, loss {cumulated_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing using the train_test_split function\n",
    "from torch.utils.data import random_split\n",
    "dataset = RandomGraphDataset(root='./data', gen_num_graph=250, n=n, p=p)\n",
    "train_dataset, test_dataset = random_split(dataset, [200, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.20106768608093262, validation loss 0.1900792121887207\n",
      "Epoch 1, loss 0.08652420341968536, validation loss 0.0728931725025177\n",
      "Epoch 2, loss 1.7268340587615967, validation loss 1.5496764183044434\n",
      "Epoch 3, loss 1.7241554260253906, validation loss 1.547053575515747\n",
      "Epoch 4, loss 1.7241947650909424, validation loss 1.5469694137573242\n",
      "Epoch 5, loss 1.7242014408111572, validation loss 1.5469566583633423\n",
      "Epoch 6, loss 0.06705305725336075, validation loss 0.05606688931584358\n",
      "Epoch 7, loss 0.06038669869303703, validation loss 0.049940671771764755\n",
      "Epoch 8, loss 0.05842023342847824, validation loss 0.04803124815225601\n",
      "Epoch 9, loss 0.057915251702070236, validation loss 0.047455139458179474\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "model = Network()\n",
    "\n",
    "train(model=model, train_dataset=train_dataset, validation_dataset=test_dataset,\n",
    "      optimizer=optimizer(model.parameters()), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1., 1.], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i =31\n",
    "graph = test_dataset[i]\n",
    "print(graph.reach_h[-1])\n",
    "print(torch.sigmoid(model(graph)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3500525.5000, 3524301.5000, 3249399.0000, 3501471.5000, 3512054.0000,\n",
       "        3518177.7500], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(graph)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.1667, 1.0000],\n",
       "        [0.3333, 0.0000],\n",
       "        [0.5000, 0.0000],\n",
       "        [0.6667, 0.0000],\n",
       "        [0.8333, 0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0748],\n",
       "        [-0.0829],\n",
       "        [-0.0641],\n",
       "        [-0.0551],\n",
       "        [-0.0525],\n",
       "        [-0.0458]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Network().forward(input, dataset[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lin = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(2)\n",
    "z = encoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(input.size(0), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 256])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_ = torch.cat([z, h], dim=1)\n",
    "z_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear\n",
    "\n",
    "class MPNN(MessagePassing):\n",
    "  def __init__(self, in_channels, hidden_channels, activation=None):\n",
    "    super(MPNN, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
    "    self.in_channels = in_channels\n",
    "    self.hidden_channels = hidden_channels\n",
    "    self.messages = Linear(self.in_channels * 2, self.hidden_channels)\n",
    "    self.update_fn = Linear(self.in_channels + self.hidden_channels, self.hidden_channels)\n",
    "    self.activation = activation\n",
    "\n",
    "    self.mlp = torch.nn.Sequential(\n",
    "        Linear(hidden_channels, hidden_channels),\n",
    "        torch.nn.ReLU(),\n",
    "        Linear(hidden_channels, self.hidden_channels)\n",
    "    )\n",
    "    \n",
    "  def forward(self, x, edge_index):\n",
    "    out = self.propagate(edge_index, x=x)\n",
    "    out = self.mlp(out)\n",
    "    if self.activation is not None:\n",
    "      out = self.activation(out)\n",
    "    return out\n",
    "    \n",
    "  def message(self, x_i, x_j):\n",
    "    # x_i has shape [E, in_channels]\n",
    "    # x_j has shape [E, in_channels]\n",
    "    #print('MPNN => xi, xj', x_i.size(), x_j.size())\n",
    "    tmp = torch.cat([x_i, x_j], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "    #print('MPNN => messages IN', tmp.size())\n",
    "    m = self.messages(tmp)\n",
    "    #print('MPNN => messages OUT', m.size())\n",
    "    return m\n",
    "  \n",
    "  def update(self, aggr_out, x):\n",
    "    # aggr_out has shape [N, out_channels]\n",
    "    # x has shape [N, in_channels]\n",
    "    #print(f'MPNN => x_i', x.size(), ' aggr_out ', aggr_out.size())\n",
    "    tmp = torch.cat([x, aggr_out], dim=1)\n",
    "    #print(f'MPNN => tmp', tmp.size())\n",
    "    return self.update_fn(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MPNN(256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = processor(z_, dataset[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
